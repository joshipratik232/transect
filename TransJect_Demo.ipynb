{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557d5989",
   "metadata": {},
   "source": [
    "# TransJect Demo: Knowledge Transfer Framework\n",
    "\n",
    "This notebook demonstrates the complete functionality of TransJect, a novel knowledge transfer framework for neural networks.\n",
    "\n",
    "## Features Demonstrated:\n",
    "1. **SequenceClassification** - Classification tasks (CB dataset from SuperGLUE)\n",
    "2. **AutoModel** - Language modeling tasks (Alpaca dataset)\n",
    "3. **Meta-Learning** - Using multiple meta dataloaders\n",
    "4. **WandB Integration** - Logging to Weights & Biases\n",
    "5. **Model Saving** - Saving trained models\n",
    "6. **Layer Slicing** - Using student_layers parameter\n",
    "\n",
    "## Setup Instructions for Google Colab:\n",
    "```python\n",
    "# Clone the repository\n",
    "!git clone https://github.com/yourusername/transject.git\n",
    "%cd transject\n",
    "\n",
    "# Install the package\n",
    "!pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f382d41b",
   "metadata": {},
   "source": [
    "## üì¶ Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a356322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab: Upload the transject folder or install from pip\n",
    "# Option 1: If running locally, just import\n",
    "# Option 2: If on Colab, install dependencies\n",
    "\n",
    "!pip install torch transformers datasets numpy tqdm scipy wandb -q\n",
    "\n",
    "# Import sys to add local path if needed\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path if running from examples folder\n",
    "if os.path.exists('../transject'):\n",
    "    sys.path.insert(0, '..')\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TransJect modules\n",
    "from transject import SequenceClassification, AutoModel, TransJectConfig\n",
    "from transject.data_utils import (\n",
    "    create_superglue_dataloaders,\n",
    "    create_alpaca_dataloaders,\n",
    "    create_meta_dataloaders\n",
    ")\n",
    "\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"‚úÖ TransJect imported successfully!\")\n",
    "print(f\"üîß PyTorch version: {torch.__version__}\")\n",
    "print(f\"üñ•Ô∏è  Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75065ae8",
   "metadata": {},
   "source": [
    "## üéØ Part 1: Sequence Classification with CB Dataset\n",
    "\n",
    "We'll use the CommitmentBank (CB) dataset from SuperGLUE for a 3-class classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d29f2",
   "metadata": {},
   "source": [
    "### 1.1 Basic Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddef4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SequenceClassification model\n",
    "print(\"üöÄ Creating SequenceClassification model...\")\n",
    "\n",
    "model_classification = SequenceClassification(\n",
    "    student_model=\"distilbert-base-uncased\",\n",
    "    teacher_model=\"bert-base-uncased\",\n",
    "    num_labels=3,  # CB has 3 labels\n",
    "    student_layers=-1,  # -1 means use full model (no slicing)\n",
    "    temperature=2.0,\n",
    "    alpha=0.5,  # Balance between distillation and task loss\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=50,\n",
    "    log_interval=5,\n",
    "    eval_interval=20\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model created successfully!\")\n",
    "print(f\"üìä Student model: {model_classification.student_model_name}\")\n",
    "print(f\"üë®‚Äçüè´ Teacher model: {model_classification.teacher_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29add12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for CB task\n",
    "print(\"üìö Loading CB dataset...\")\n",
    "\n",
    "train_loader_cb, val_loader_cb, num_labels = create_superglue_dataloaders(\n",
    "    task_name=\"cb\",\n",
    "    tokenizer=model_classification.tokenizer,\n",
    "    batch_size=8,\n",
    "    max_length=128,\n",
    "    num_train_samples=200,  # Use subset for quick demo\n",
    "    num_val_samples=50\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataloaders created!\")\n",
    "print(f\"üìä Training batches: {len(train_loader_cb)}\")\n",
    "print(f\"üìä Validation batches: {len(val_loader_cb)}\")\n",
    "print(f\"üè∑Ô∏è  Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (without W&B logging for simplicity)\n",
    "print(\"üèãÔ∏è Training classification model...\\n\")\n",
    "\n",
    "model_classification.fit(\n",
    "    train_dataloader=train_loader_cb,\n",
    "    val_dataloader=val_loader_cb,\n",
    "    epochs=2,  # Short training for demo\n",
    "    report_to=None,  # Set to \"wandb\" if you want W&B logging\n",
    "    output_dir=\"./output/cb_basic\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d05ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained student model\n",
    "print(\"üíæ Saving trained model...\")\n",
    "\n",
    "output_path = \"./output/cb_basic/final_student_model\"\n",
    "model_classification.student_model.save_pretrained(output_path)\n",
    "model_classification.tokenizer.save_pretrained(output_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb762d6d",
   "metadata": {},
   "source": [
    "### 1.2 Classification with Meta-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb4f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model with meta-learning enabled\n",
    "print(\"üöÄ Creating model with meta-learning...\")\n",
    "\n",
    "model_classification_meta = SequenceClassification(\n",
    "    student_model=\"distilbert-base-uncased\",\n",
    "    teacher_model=\"bert-base-uncased\",\n",
    "    num_labels=3,\n",
    "    student_layers=-1,\n",
    "    use_meta_learning=True,\n",
    "    meta_learning_rate=1e-4\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model with meta-learning created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af125eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create meta-learning dataloaders\n",
    "print(\"üìö Creating meta-learning dataloaders...\")\n",
    "\n",
    "meta_loaders = create_meta_dataloaders(\n",
    "    tokenizer=model_classification_meta.tokenizer,\n",
    "    tasks=[\"rte\", \"wic\"],  # Additional SuperGLUE tasks\n",
    "    batch_size=8,\n",
    "    max_length=128,\n",
    "    num_samples_per_task=50  # Small subset for demo\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Created {len(meta_loaders)} meta-dataloaders: {list(meta_loaders.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with meta-learning\n",
    "print(\"üèãÔ∏è Training with meta-learning...\\n\")\n",
    "\n",
    "model_classification_meta.fit(\n",
    "    train_dataloader=train_loader_cb,\n",
    "    meta_dataloader=meta_loaders,  # Dictionary of meta-loaders\n",
    "    val_dataloader=val_loader_cb,\n",
    "    epochs=2,\n",
    "    report_to=None,\n",
    "    output_dir=\"./output/cb_meta\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Meta-learning training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac7ba0",
   "metadata": {},
   "source": [
    "## ü§ñ Part 2: Language Modeling with Alpaca Dataset\n",
    "\n",
    "Now we'll demonstrate language modeling using the Alpaca instruction-following dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317119e4",
   "metadata": {},
   "source": [
    "### 2.1 Basic Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1278e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an AutoModel for language modeling\n",
    "print(\"üöÄ Creating AutoModel for language modeling...\")\n",
    "\n",
    "model_lm = AutoModel(\n",
    "    student_model=\"gpt2\",\n",
    "    teacher_model=\"gpt2-medium\",  # Using medium as teacher\n",
    "    student_layers=-1,  # Use full GPT-2 model\n",
    "    temperature=2.0,\n",
    "    alpha=0.5,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=50,\n",
    "    log_interval=5,\n",
    "    eval_interval=20\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Language model created successfully!\")\n",
    "print(f\"üìä Student model: {model_lm.student_model_name}\")\n",
    "print(f\"üë®‚Äçüè´ Teacher model: {model_lm.teacher_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Alpaca dataloaders\n",
    "print(\"üìö Loading Alpaca dataset...\")\n",
    "\n",
    "try:\n",
    "    train_loader_alpaca, val_loader_alpaca = create_alpaca_dataloaders(\n",
    "        tokenizer=model_lm.tokenizer,\n",
    "        batch_size=4,  # Smaller batch size for language modeling\n",
    "        max_length=256,  # Shorter sequences for demo\n",
    "        num_train_samples=100,  # Small subset for quick demo\n",
    "        num_val_samples=20,\n",
    "        dataset_name=\"tatsu-lab/alpaca\"  # Official Alpaca dataset\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Alpaca dataloaders created!\")\n",
    "    print(f\"üìä Training batches: {len(train_loader_alpaca)}\")\n",
    "    print(f\"üìä Validation batches: {len(val_loader_alpaca)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load Alpaca dataset: {e}\")\n",
    "    print(\"üí° You can use a custom dataset or skip this section\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a29ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the language model\n",
    "print(\"üèãÔ∏è Training language model...\\n\")\n",
    "\n",
    "model_lm.fit(\n",
    "    train_dataloader=train_loader_alpaca,\n",
    "    val_dataloader=val_loader_alpaca,\n",
    "    epochs=1,  # Just 1 epoch for demo\n",
    "    report_to=None,  # Set to \"wandb\" for W&B logging\n",
    "    output_dir=\"./output/alpaca_basic\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Language model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4a07ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text generation with the trained model\n",
    "print(\"üé® Testing text generation...\\n\")\n",
    "\n",
    "prompt = \"### Instruction:\\nWrite a short poem about AI\\n\\n### Response:\\n\"\n",
    "inputs = model_lm.tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model_lm.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "generated_text = model_lm.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated text:\")\n",
    "print(\"=\"*50)\n",
    "print(generated_text)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a2f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained language model\n",
    "print(\"üíæ Saving trained language model...\")\n",
    "\n",
    "output_path_lm = \"./output/alpaca_basic/final_student_model\"\n",
    "model_lm.student_model.save_pretrained(output_path_lm)\n",
    "model_lm.tokenizer.save_pretrained(output_path_lm)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {output_path_lm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb3a66c",
   "metadata": {},
   "source": [
    "### 2.2 Language Modeling with Layer Slicing\n",
    "\n",
    "Demonstrating the `student_layers` parameter to use only a subset of layers from the student model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dbd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with layer slicing\n",
    "print(\"üöÄ Creating model with layer slicing...\")\n",
    "\n",
    "model_lm_sliced = AutoModel(\n",
    "    student_model=\"gpt2-medium\",  # Start with GPT2-medium\n",
    "    teacher_model=\"gpt2-large\",   # Teacher is GPT2-large\n",
    "    student_layers=12,  # Use only first 12 layers (GPT2-medium has 24)\n",
    "    temperature=2.0,\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model with layer slicing created!\")\n",
    "print(f\"üìä Using first 12 layers of {model_lm_sliced.student_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf1a38",
   "metadata": {},
   "source": [
    "## üìä Part 3: Advanced Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a28ad",
   "metadata": {},
   "source": [
    "### 3.1 Using Custom Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63477d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom configuration\n",
    "from transject import TransJectConfig\n",
    "\n",
    "custom_config = TransJectConfig(\n",
    "    student_layers=-1,\n",
    "    temperature=3.0,  # Higher temperature\n",
    "    alpha=0.7,  # More weight on distillation\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=200,\n",
    "    max_grad_norm=1.0,\n",
    "    accumulation_steps=2,  # Gradient accumulation\n",
    "    use_meta_learning=True,\n",
    "    meta_learning_rate=5e-5,\n",
    "    log_interval=10,\n",
    "    eval_interval=50,\n",
    "    save_interval=200,\n",
    "    fp16=True,  # Mixed precision training (if CUDA available)\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Save configuration\n",
    "custom_config.to_json(\"./output/my_config.json\")\n",
    "\n",
    "print(\"‚úÖ Custom configuration created and saved!\")\n",
    "print(f\"üìä Config: {custom_config.to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1229fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with Llama models (requires HF token and model access)\n",
    "# First, login to HuggingFace\n",
    "\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Get your HuggingFace token\n",
    "# Option 1: Set as environment variable\n",
    "# HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Option 2: Use userdata in Colab\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "except:\n",
    "    # Option 3: Paste your token here (not recommended for public repos)\n",
    "    HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"  # Replace with your actual token\n",
    "    print(\"‚ö†Ô∏è  Warning: Replace YOUR_HF_TOKEN_HERE with your actual token\")\n",
    "    print(\"üí° Better: Use Colab Secrets (left sidebar > üîë key icon) to store HF_TOKEN\")\n",
    "\n",
    "# Login to HuggingFace\n",
    "if HF_TOKEN and HF_TOKEN != \"YOUR_HF_TOKEN_HERE\":\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úÖ Logged in to HuggingFace!\")\n",
    "else:\n",
    "    print(\"‚ùå Please set your HuggingFace token\")\n",
    "    print(\"üìù Get your token from: https://huggingface.co/settings/tokens\")\n",
    "\n",
    "# Now create Llama model with token\n",
    "print(\"\\nüöÄ Creating Llama-3-8B model...\")\n",
    "\n",
    "model_llama = AutoModel(\n",
    "    student_model=\"meta-llama/Meta-Llama-3-8B\",\n",
    "    teacher_model=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    student_layers=-1,  # Use full Llama-3-8B\n",
    "    temperature=2.0,\n",
    "    alpha=0.5,\n",
    "    token=HF_TOKEN  # Pass token for gated model access\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Llama model created successfully!\")\n",
    "print(f\"üìä Student: {model_llama.student_model_name}\")\n",
    "print(f\"üë®‚Äçüè´ Teacher: {model_llama.teacher_model_name}\")\n",
    "\n",
    "# Test generation\n",
    "prompt = \"### Instruction:\\nWrite a haiku about AI\\n\\n### Response:\\n\"\n",
    "inputs = model_llama.tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_llama.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "generated = model_llama.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\nüìù Generated text:\")\n",
    "print(\"=\"*60)\n",
    "print(generated)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee746df",
   "metadata": {},
   "source": [
    "### 3.2 Training with W&B Logging\n",
    "\n",
    "To use Weights & Biases logging, uncomment and run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize W&B (uncomment to use)\n",
    "# import wandb\n",
    "# \n",
    "# wandb.login()  # You'll need to paste your API key\n",
    "# \n",
    "# wandb.init(\n",
    "#     project=\"transject-demo\",\n",
    "#     name=\"cb-classification\",\n",
    "#     config={\n",
    "#         \"student_model\": \"distilbert-base-uncased\",\n",
    "#         \"teacher_model\": \"bert-base-uncased\",\n",
    "#         \"task\": \"cb\",\n",
    "#         \"epochs\": 3\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fe238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train with W&B logging (uncomment to use)\n",
    "# model_classification.fit(\n",
    "#     train_dataloader=train_loader_cb,\n",
    "#     val_dataloader=val_loader_cb,\n",
    "#     epochs=3,\n",
    "#     report_to=\"wandb\",  # Enable W&B logging\n",
    "#     output_dir=\"./output/cb_wandb\"\n",
    "# )\n",
    "# \n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6949fb2",
   "metadata": {},
   "source": [
    "### 3.3 Working with Llama Models\n",
    "\n",
    "TransJect supports modern LLMs like Llama-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260558a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example with Llama models (requires HF token and model access)\n",
    "# # Uncomment to use\n",
    "# \n",
    "# model_llama = AutoModel(\n",
    "#     student_model=\"meta-llama/Llama-3-8B\",\n",
    "#     teacher_model=\"meta-llama/Llama-3-70B\",  # If you have access\n",
    "#     student_layers=-1,  # Use full Llama-3-8B\n",
    "#     temperature=2.0,\n",
    "#     alpha=0.5\n",
    "# )\n",
    "# \n",
    "# print(\"‚úÖ Llama model created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa428e",
   "metadata": {},
   "source": [
    "## üéì Part 4: Loading Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a6797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved student model\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "print(\"üìÇ Loading saved model...\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"./output/cb_basic/final_student_model\"\n",
    ")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"./output/cb_basic/final_student_model\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# Test inference\n",
    "test_text = (\"The economy is improving.\", \"The financial situation is getting better.\")\n",
    "inputs = loaded_tokenizer(*test_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(f\"\\nüìä Test inference:\")\n",
    "print(f\"Premise: {test_text[0]}\")\n",
    "print(f\"Hypothesis: {test_text[1]}\")\n",
    "print(f\"Prediction: {predictions.item()} (0=entailment, 1=contradiction, 2=neutral)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8566e4a",
   "metadata": {},
   "source": [
    "## üìù Summary\n",
    "\n",
    "In this demo, we covered:\n",
    "\n",
    "### ‚úÖ Classification Tasks\n",
    "- Basic sequence classification with CB dataset\n",
    "- Meta-learning with multiple tasks\n",
    "- Model saving and loading\n",
    "\n",
    "### ‚úÖ Language Modeling Tasks\n",
    "- Training on Alpaca instruction dataset\n",
    "- Text generation with trained models\n",
    "- Layer slicing for efficient knowledge transfer\n",
    "\n",
    "### ‚úÖ Advanced Features\n",
    "- Custom configuration\n",
    "- W&B integration (optional)\n",
    "- Support for modern LLMs (Llama-3, etc.)\n",
    "\n",
    "### üéØ Key API Pattern\n",
    "\n",
    "```python\n",
    "# 1. Create model\n",
    "model = SequenceClassification(  # or AutoModel\n",
    "    student_model=\"model-name\",\n",
    "    teacher_model=\"teacher-name\",\n",
    "    student_layers=-1,  # -1 for full model\n",
    "    **config_params\n",
    ")\n",
    "\n",
    "# 2. Create dataloaders\n",
    "train_loader, val_loader = create_dataloaders(...)\n",
    "meta_loaders = create_meta_dataloaders(...)  # Optional\n",
    "\n",
    "# 3. Train\n",
    "model.fit(\n",
    "    train_dataloader=train_loader,\n",
    "    meta_dataloader=meta_loaders,  # Optional\n",
    "    val_dataloader=val_loader,\n",
    "    epochs=3,\n",
    "    report_to=\"wandb\"  # Optional\n",
    ")\n",
    "\n",
    "# 4. Save\n",
    "model.student_model.save_pretrained(\"path\")\n",
    "```\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "1. Try different model combinations\n",
    "2. Experiment with hyperparameters\n",
    "3. Use your own datasets\n",
    "4. Enable W&B logging for better tracking\n",
    "5. Try layer slicing for efficient training\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [GitHub Repository](https://github.com/transject/transject)\n",
    "- [Documentation](https://transject.readthedocs.io)\n",
    "- [Paper (coming soon)]()\n",
    "\n",
    "Happy knowledge transferring! üéì"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
